{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'tensorflow==1.5.0'\n",
    "# !pip install 'edward==1.3.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShvVUXv8kDu7"
   },
   "source": [
    "# Configure env..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DXjxHH6et_yc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:  1592223649\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import numpy.random as npr\n",
    "import os\n",
    "from datetime import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from scipy.stats import invgamma\n",
    "from edward.models import Normal, Gamma, Dirichlet, InverseGamma, \\\n",
    "    Poisson, PointMass, Empirical, ParamMixture, \\\n",
    "    MultivariateNormalDiag, Categorical, Laplace,\\\n",
    "    MultivariateNormalTriL, Bernoulli, TransformedDistribution, \\\n",
    "    Binomial\n",
    "from edward.util import Progbar\n",
    "from scipy import sparse, stats\n",
    "from scipy.special import expit, logit\n",
    "# from deconfounder_poissonMF import PoissonMF\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.10f}\".format(x)})\n",
    "\n",
    "# set random seed so everyone gets the same number\n",
    "import random\n",
    "import time\n",
    "randseed = int(time.time())\n",
    "# random seed for reproducibility\n",
    "randseed = 1592223649\n",
    "print(\"random seed: \", randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'data')\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(randseed)\n",
    "np.random.seed(randseed)\n",
    "tf.set_random_seed(randseed)\n",
    "\n",
    "N=5000\n",
    "K=10\n",
    "D=50\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "# Simulate causes and confounders\n",
    "C = np.random.normal(0, 1, size = [N, K])\n",
    "lambd = np.random.normal(0, 0.5, size = [K, D])\n",
    "bernoulli_p = sigmoid(np.dot(C, lambd))                \n",
    "A = np.random.binomial(1, bernoulli_p, size = bernoulli_p.shape)\n",
    "\n",
    "# To ensure reproducibility we load the cause and confounder matrices saved previously\n",
    "A = np.loadtxt(os.path.join(DATA_PATH, 'simulated_causes.txt'))\n",
    "C = np.loadtxt(os.path.join(DATA_PATH, 'simulated_multicause_conf.txt'))\n",
    "\n",
    "# Simulate sets of coefficients and outcomes\n",
    "Nsim = 500\n",
    "betas = np.zeros((Nsim, D))\n",
    "gammas = np.zeros((Nsim, K))\n",
    "Ys = np.zeros((Nsim, N))\n",
    "for sim in range(Nsim):\n",
    "    beta = np.random.normal(0, 0.25, size = D)\n",
    "    zero_coeff_idx = np.random.choice(np.arange(len(beta)), size = 40, replace = False)\n",
    "    beta[zero_coeff_idx] = 0\n",
    "\n",
    "    gamma = np.random.normal(0, 0.25, size = K)\n",
    "    \n",
    "    noise = np.random.normal(0, 1, size = N).reshape(-1,1)\n",
    "    Y = np.dot(A, beta.reshape(D,1)) + np.dot(C, gamma.reshape(10,1)) + noise\n",
    "    \n",
    "    betas[sim,:] = beta\n",
    "    gammas[sim,:] = gamma\n",
    "    Ys[sim, np.newaxis] = Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(DATA_PATH, 'simulated_causes.txt'), A)\n",
    "np.savetxt(os.path.join(DATA_PATH, 'simulated_multicause_conf.txt'), C)\n",
    "np.savetxt(os.path.join(DATA_PATH, 'simulated_outcomes.txt'), Ys)\n",
    "np.savetxt(os.path.join(DATA_PATH, 'simulated_true_coeffs.txt'), betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the cause matrix\n",
    "X = A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMF deconfounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly holdout some entries of X\n",
    "num_datapoints, data_dim = X.shape\n",
    "\n",
    "holdout_portion = 0.5\n",
    "n_holdout = int(holdout_portion * num_datapoints * data_dim)\n",
    "\n",
    "holdout_row = np.random.randint(num_datapoints, size=n_holdout)\n",
    "holdout_col = np.random.randint(data_dim, size=n_holdout)\n",
    "holdout_mask = (sparse.coo_matrix((np.ones(n_holdout), \\\n",
    "                            (holdout_row, holdout_col)), \\\n",
    "                            shape = X.shape)).toarray()\n",
    "holdout_mask = np.minimum(holdout_mask, np.ones(X.shape))\n",
    "holdout_mask = np.float32(holdout_mask)\n",
    "\n",
    "\n",
    "holdout_subjects = np.unique(holdout_row)\n",
    "\n",
    "x_train = np.multiply(1-holdout_mask, X)\n",
    "x_vad = np.multiply(holdout_mask, X)\n",
    "\n",
    "M = 10  # minibatch size\n",
    "# for stochastic optimization\n",
    "# subsample datapoints\n",
    "def next_batch(x_train, M):\n",
    "    idx_batch = np.random.choice(N, M)\n",
    "    return x_train[idx_batch, :], idx_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498/500 [ 99%] █████████████████████████████  ETA: 0sPredictive check p-values 0.496\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K_list = [450]\n",
    "K_pval = []\n",
    "for K in K_list:\n",
    "    train_data = np.array(x_train, dtype=int)\n",
    "    D = train_data.shape[1]\n",
    "    N = train_data.shape[0]\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    idx_ph = tf.placeholder(tf.int32, M)\n",
    "    x_ph = tf.placeholder(tf.float32, [M, D])\n",
    "\n",
    "    U = Gamma(0.1, 0.5, sample_shape=[M,K])\n",
    "    V = Gamma(0.1, 0.3, sample_shape=[D,K])\n",
    "    x = Poisson(tf.matmul(U, V, transpose_b=True))\n",
    "\n",
    "    min_scale = 1e-5\n",
    "\n",
    "    qV_variables = [tf.Variable(tf.random_uniform([D, K])), \\\n",
    "                    tf.Variable(tf.random_uniform([D, K]))]\n",
    "\n",
    "    qV = TransformedDistribution(\n",
    "                distribution=Normal(qV_variables[0],\\\n",
    "                                    tf.maximum(tf.nn.softplus(qV_variables[1]), \\\n",
    "                                               min_scale)),\n",
    "                bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "\n",
    "\n",
    "    qU_variables = [tf.Variable(tf.random_uniform([N, K])), \\\n",
    "                    tf.Variable(tf.random_uniform([N, K]))]\n",
    "\n",
    "\n",
    "    qU = TransformedDistribution(\n",
    "                distribution=Normal(tf.gather(qU_variables[0], idx_ph),\\\n",
    "                                    tf.maximum(tf.nn.softplus(tf.gather(qU_variables[1], idx_ph)), \\\n",
    "                                               min_scale)),\n",
    "                bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "\n",
    "\n",
    "    scale_factor = float(N) / M\n",
    "\n",
    "    # We apply variational EM with E-step over local variables\n",
    "    # and M-step to point estimate the global weight matrices.\n",
    "    inference_e = ed.KLqp({U: qU},\n",
    "                        data={x: x_ph, V:qV})\n",
    "    inference_m = ed.KLqp({V:qV},\n",
    "                       data={x: x_ph, U:qU})\n",
    "\n",
    "    optimizer_e = tf.train.RMSPropOptimizer(1e-4)\n",
    "    optimizer_m = tf.train.RMSPropOptimizer(1e-4)\n",
    "\n",
    "    inference_e.initialize(scale={x: scale_factor, U: scale_factor}, var_list=qU_variables, optimizer=\"rmsprop\")\n",
    "    inference_m.initialize(scale={x: scale_factor, U: scale_factor}, optimizer=\"rmsprop\")\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    loss = []\n",
    "    n_epoch = 20\n",
    "    n_iter_per_epoch = 500\n",
    "    for epoch in range(n_epoch):\n",
    "    #     print(\"Epoch {}\".format(epoch))\n",
    "        nll = 0.0\n",
    "\n",
    "        pbar = Progbar(n_iter_per_epoch)\n",
    "        for t in range(n_iter_per_epoch):\n",
    "            x_batch, idx_batch = next_batch(train_data, M)\n",
    "    #         x_batch = x_batch.todense()\n",
    "            pbar.update(t)\n",
    "            info_dict_e = inference_e.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n",
    "            info_dict_m = inference_m.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n",
    "            nll += info_dict_e['loss']\n",
    "    #         print('\\n'+str(info_dict_e['loss']))    \n",
    "            loss.append(info_dict_e['loss'])\n",
    "\n",
    "    pd.Series(loss).plot()\n",
    "\n",
    "    V_post = TransformedDistribution(\n",
    "                distribution=Normal(qV_variables[0],\\\n",
    "                                    tf.maximum(tf.nn.softplus(qV_variables[1]), \\\n",
    "                                               min_scale)),\n",
    "                bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "    U_post = TransformedDistribution(\n",
    "                distribution=Normal(qU_variables[0],\\\n",
    "                                    tf.maximum(tf.nn.softplus(qU_variables[1]), \\\n",
    "                                               min_scale)),\n",
    "                bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "\n",
    "    pmf_conf = U_post\n",
    "\n",
    "    x_post = Poisson(tf.matmul(U_post, V_post, transpose_b=True))\n",
    "\n",
    "    ### predictive check\n",
    "\n",
    "    n_rep = 100 # number of replicated datasets we generate\n",
    "    holdout_gen = np.zeros((n_rep, x_train.shape[0], x_train.shape[1]))\n",
    "    \n",
    "    for i in range(n_rep):\n",
    "        x_generated = x_post.sample().eval()\n",
    "\n",
    "        # look only at the heldout entries\n",
    "        holdout_gen[i] = np.multiply(x_generated, holdout_mask)\n",
    "\n",
    "    n_eval = 10 # we draw samples from the inferred Z and W\n",
    "    obs_ll = []\n",
    "    rep_ll = []\n",
    "    for j in range(n_eval):\n",
    "        U_sample = U_post.sample().eval()\n",
    "        V_sample = V_post.sample().eval()\n",
    "\n",
    "        holdoutmean_sample = np.multiply(U_sample.dot(V_sample.T), holdout_mask)\n",
    "        obs_ll.append(\\\n",
    "            np.mean(np.ma.masked_invalid(stats.poisson.logpmf(np.array(x_vad, dtype=int), \\\n",
    "                                                              holdoutmean_sample)), axis=0))\n",
    "\n",
    "        rep_ll.append(\\\n",
    "            np.mean(np.ma.masked_invalid(stats.poisson.logpmf(holdout_gen, \\\n",
    "                                                              holdoutmean_sample)), axis=1))\n",
    "\n",
    "    obs_ll_per_zi, rep_ll_per_zi = np.mean(np.array(obs_ll), axis=0), np.mean(np.array(rep_ll), axis=0)\n",
    "\n",
    "    pvals = np.array([np.mean(rep_ll_per_zi[:,i] < obs_ll_per_zi[i]) for i in range(len(obs_ll_per_zi))])\n",
    "    holdout_subjects = np.unique(holdout_col)\n",
    "    overall_pval = np.mean(pvals[holdout_subjects])\n",
    "    print(\"Predictive check p-values\", overall_pval)\n",
    "    K_pval.append(overall_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save result\n",
    "x_post_np = x_post.mean().eval()\n",
    "pmf_Z_post_np = pmf_conf.eval()\n",
    "\n",
    "np.savetxt(os.path.join(DATA_PATH, \"x_post_np_PMF_k450.txt\"), x_post_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconfounder: DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 50\n",
      "Epoch 0\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 391532.458\n",
      "Perplexity <= 175.706\n",
      "Epoch 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 207840.734\n",
      "Perplexity <= 15.546\n",
      "Epoch 2\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 185318.102\n",
      "Perplexity <= 11.548\n",
      "Epoch 3\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s\n",
      "Negative log-likelihood <= 181456.758\n",
      "Perplexity <= 10.974\n",
      "Epoch 4\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 180486.477\n",
      "Perplexity <= 10.834\n",
      "Epoch 5\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 179707.286\n",
      "Perplexity <= 10.723\n",
      "Epoch 6\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179957.161\n",
      "Perplexity <= 10.759\n",
      "Epoch 7\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 179873.423\n",
      "Perplexity <= 10.747\n",
      "Epoch 8\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 6s\n",
      "Negative log-likelihood <= 179360.266\n",
      "Perplexity <= 10.674\n",
      "Epoch 9\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179872.561\n",
      "Perplexity <= 10.747\n",
      "Epoch 10\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179643.783\n",
      "Perplexity <= 10.714\n",
      "Epoch 11\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179457.401\n",
      "Perplexity <= 10.688\n",
      "Epoch 12\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179948.236\n",
      "Perplexity <= 10.757\n",
      "Epoch 13\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179882.378\n",
      "Perplexity <= 10.748\n",
      "Epoch 14\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 180013.716\n",
      "Perplexity <= 10.767\n",
      "Epoch 15\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179652.742\n",
      "Perplexity <= 10.715\n",
      "Epoch 16\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179630.224\n",
      "Perplexity <= 10.712\n",
      "Epoch 17\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "Negative log-likelihood <= 179576.899\n",
      "Perplexity <= 10.705\n",
      "Epoch 18\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 3s\n",
      "Negative log-likelihood <= 179708.980\n",
      "Perplexity <= 10.723\n",
      "Epoch 19\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 5s\n",
      "Negative log-likelihood <= 179559.157\n",
      "Perplexity <= 10.702\n"
     ]
    }
   ],
   "source": [
    "N = x_train.shape[0]  # number of data points\n",
    "D = x_train.shape[1]  # data dimensionality\n",
    "min_scale = 1e-5\n",
    "print (N, D)\n",
    "train_data = np.array(x_train, dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "q = 'lognormal'  # choice of q; 'lognormal' or 'gamma'\n",
    "shape = 0.1  # gamma shape parameter\n",
    "lr = 1e-3  # learning rate step-size\n",
    "logdir = '~/log/def/'\n",
    "logdir = os.path.expanduser(logdir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# note this x matrix is transpose of the other matrices in PPCA/PMF\n",
    "x_ph = tf.placeholder(tf.float32, [M, D])\n",
    "idx_ph = tf.placeholder(tf.int32, M)\n",
    "\n",
    "\n",
    "# MODEL\n",
    "Ks = [2, 2]  #\n",
    "W1 = Gamma(0.1, 0.3, sample_shape=[Ks[1], Ks[0]])\n",
    "W0 = Gamma(0.1, 0.3, sample_shape=[Ks[0], D])\n",
    "\n",
    "z2 = Gamma(0.1, 0.1, sample_shape=[M, Ks[1]])\n",
    "z1 = Gamma(shape, shape / tf.matmul(z2, W1))\n",
    "\n",
    "x = Poisson(tf.matmul(z1, W0))\n",
    "\n",
    "\n",
    "# INFERENCE\n",
    "def pointmass_q(shape):\n",
    "    min_mean = 1e-3\n",
    "    mean_init = tf.random_normal(shape)\n",
    "    rv = PointMass(tf.maximum(tf.nn.softplus(tf.Variable(mean_init)), min_mean))\n",
    "    return rv\n",
    "\n",
    "\n",
    "def gamma_q(shape):\n",
    "    # Parameterize Gamma q's via shape and scale, with softplus unconstraints.\n",
    "    min_shape = 1e-3\n",
    "    min_scale = 1e-5\n",
    "    shape_init = 0.5 + 0.1 * tf.random_normal(shape)\n",
    "    scale_init = 0.1 * tf.random_normal(shape)\n",
    "    rv = Gamma(tf.maximum(tf.nn.softplus(tf.Variable(shape_init)),\n",
    "                        min_shape),\n",
    "             tf.maximum(1.0 / tf.nn.softplus(tf.Variable(scale_init)),\n",
    "                        1.0 / min_scale))\n",
    "    return rv\n",
    "\n",
    "\n",
    "def lognormal_q(shape):\n",
    "    min_scale = 1e-5\n",
    "    loc_init = tf.random_normal(shape)\n",
    "    scale_init = 0.1 * tf.random_normal(shape)\n",
    "    rv = TransformedDistribution(\n",
    "      distribution=Normal(\n",
    "          tf.Variable(loc_init),\n",
    "          tf.maximum(tf.nn.softplus(tf.Variable(scale_init)), min_scale)),\n",
    "      bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "    return rv\n",
    "\n",
    "\n",
    "\n",
    "qW1 = pointmass_q(W1.shape)\n",
    "qW0 = pointmass_q(W0.shape)\n",
    "\n",
    "\n",
    "qz2 = lognormal_q(z2.shape)\n",
    "qz1 = lognormal_q(z1.shape)\n",
    "\n",
    "qz2_variables = [tf.Variable(tf.random_uniform([N, Ks[1]])), \\\n",
    "                tf.Variable(tf.random_uniform([N, Ks[1]]))]\n",
    "\n",
    "\n",
    "qz2 = TransformedDistribution(\n",
    "            distribution=Normal(tf.gather(qz2_variables[0], idx_ph),\\\n",
    "                                tf.maximum(tf.nn.softplus(tf.gather(qz2_variables[1], idx_ph)), \\\n",
    "                                           min_scale)),\n",
    "            bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "\n",
    "# We apply variational EM with E-step over local variables\n",
    "# and M-step to point estimate the global weight matrices.\n",
    "inference_e = ed.KLqp({z1: qz1, z2: qz2},\n",
    "                      data={x: x_ph, W0: qW0, W1: qW1})\n",
    "inference_m = ed.MAP({W0: qW0, W1: qW1},\n",
    "                     data={x: x_ph, z1: qz1, z2: qz2})\n",
    "\n",
    "scale_factor = float(N) / M\n",
    "\n",
    "\n",
    "optimizer_e = tf.train.RMSPropOptimizer(lr)\n",
    "optimizer_m = tf.train.RMSPropOptimizer(lr)\n",
    "timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
    "logdir += timestamp + '_' + '_'.join([str(ks) for ks in Ks]) + \\\n",
    "    '_q_' + str(q) + '_lr_' + str(lr)\n",
    "kwargs = {'optimizer': optimizer_e,\n",
    "          'n_print': 100,\n",
    "          'logdir': logdir,\n",
    "          'log_timestamp': False, \n",
    "          'scale': {x: scale_factor, z2: scale_factor}}\n",
    "\n",
    "if q == 'gamma':\n",
    "    kwargs['n_samples'] = 30\n",
    "inference_e.initialize(**kwargs)\n",
    "inference_m.initialize(optimizer=optimizer_m, \\\n",
    "                       scale = {x: scale_factor, z2: scale_factor})\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "loss = []\n",
    "n_epoch = 20\n",
    "n_iter_per_epoch = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    nll = 0.0\n",
    "\n",
    "    pbar = Progbar(n_iter_per_epoch)\n",
    "    for t in range(1, n_iter_per_epoch + 1):\n",
    "        x_batch, idx_batch = next_batch(train_data, M)\n",
    "        # this extra step is because the x matrix is \n",
    "        # transpose of the other matrices in PPCA/PMF\n",
    "#         x_batch = x_batch.T \n",
    "        pbar.update(t)\n",
    "        info_dict_e = inference_e.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n",
    "        info_dict_m = inference_m.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n",
    "        nll += info_dict_e['loss']\n",
    "        loss.append(info_dict_e['loss'])\n",
    "\n",
    "    # Compute perplexity averaged over a number of training iterations.\n",
    "    # The model's negative log-likelihood of data is upper bounded by\n",
    "    # the variational objective.\n",
    "    nll = nll / n_iter_per_epoch\n",
    "    perplexity = np.exp(nll / np.sum(x_train))\n",
    "    print(\"Negative log-likelihood <= {:0.3f}\".format(nll))\n",
    "    print(\"Perplexity <= {:0.3f}\".format(perplexity))\n",
    "\n",
    "z2_post = TransformedDistribution(\n",
    "            distribution=Normal(qz2_variables[0],\\\n",
    "                                tf.maximum(tf.nn.softplus(qz2_variables[1]), \\\n",
    "                                           min_scale)),\n",
    "            bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "\n",
    "z1_post = Gamma(shape, shape / tf.matmul(z2_post, qW1))\n",
    "\n",
    "x_post = Poisson(tf.matmul(z1_post, qW0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [100%] ██████████████████████████████ Elapsed: 27s\n",
      "Predictive check p-values 0.614\n"
     ]
    }
   ],
   "source": [
    "n_rep = 100 # number of replicated datasets we generate\n",
    "holdout_gen = np.zeros((n_rep, x_train.shape[0], x_train.shape[1]))\n",
    "\n",
    "for i in range(n_rep):\n",
    "    x_generated = x_post.sample().eval()\n",
    "    \n",
    "    # look only at the heldout entries\n",
    "    holdout_gen[i] = np.multiply(x_generated, holdout_mask)\n",
    "\n",
    "n_eval = 10 # we draw samples from the inferred Z and W\n",
    "obs_ll = []\n",
    "rep_ll = []\n",
    "pbar = Progbar(n_eval)\n",
    "for j in range(1, n_eval+1):\n",
    "    z1_sample = z1_post.sample().eval()\n",
    "    W0_sample = qW0.sample().eval()\n",
    "    \n",
    "    holdoutmean_sample = np.multiply(z1_sample.dot(W0_sample), holdout_mask)\n",
    "    obs_ll.append(np.mean(np.ma.masked_invalid(stats.poisson.logpmf(np.array(x_vad, dtype=int), holdoutmean_sample)), axis=0))\n",
    "\n",
    "    rep_ll.append(np.mean(np.ma.masked_invalid(stats.poisson.logpmf(holdout_gen, holdoutmean_sample)), axis=1))\n",
    "    pbar.update(j)\n",
    "    \n",
    "obs_ll_per_zi, rep_ll_per_zi = np.mean(np.array(obs_ll), axis=0), np.mean(np.array(rep_ll), axis=0)\n",
    "\n",
    "pvals = np.array([np.mean(rep_ll_per_zi[:,i] < obs_ll_per_zi[i]) for i in range(len(obs_ll_per_zi))])\n",
    "holdout_subjects = np.unique(holdout_col)\n",
    "overall_pval = np.mean(pvals[holdout_subjects])\n",
    "print(\"Predictive check p-values\", overall_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct causes\n",
    "x_post_np = x_post.mean().eval()\n",
    "# drug latent variables\n",
    "def_Z_post_np = qW0.mean().eval()\n",
    "# patient latent variables\n",
    "z1_post_np = z1_post.mean().eval()\n",
    "### save result\n",
    "np.savetxt(os.path.join(DATA_PATH, \"x_post_np_DEF_2_2.txt\"), x_post_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome model in R..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rA4jMmMtfrJO",
    "UKH2LeJV6acn",
    "IZkxOn3tVoFd",
    "cG1609cc5G2-",
    "cnGaFw3TXOu4",
    "yK0rGVb9VcJK",
    "-yzXXuyCp8a0",
    "QCLRjpzf1bts"
   ],
   "default_view": {},
   "name": "latent_confounder_gene_testing.ipynb",
   "provenance": [
    {
     "file_id": "1THbZHTVUamCuyONzyWHtREtZcn163G9p",
     "timestamp": 1517254346174
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
